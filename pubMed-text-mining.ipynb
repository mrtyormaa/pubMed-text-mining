{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Big Data Finals - Word Co-occurnce in pubMed database\n",
    "##Asutosh Satapathy {asatapat@andrew.cmu.edu}\n",
    "\n",
    "###Problem Statement\n",
    "For this project, I will utilize the Pubmed Central (PMC) open access dataset, implement a system to process the PMC dataset with a user-provided term, and generate a JSON file for use in D3-driven visualization. The goal of this final is to produce a visualization of the topics that most commonly co-occur in the pubmed documents with a user provided term or phrase.\n",
    "\n",
    "###Introduction\n",
    "PubMed database contains millions of documents related to various articles. Searching through all of the dat manually is a very daunting (sort of impossible task). Hence, I wll be attempting to help explore the database with minimal effort. The basic idea is given a word, find all other words which are co-occuring. There are various ways to approach this problem. \n",
    "The approach which I have implemented is as follows: I will be scanning the entire database for this word. I will see which documents contains this word. And after that, I will be selecting the other words based on their co-occurrence in the document.\n",
    "Again, this can be done in various ways. But before I delve into the nifty details, let's first cover some of the basics of text mining which I learnt during the course of this project. This will be helpful in understanding the underlying concept of the application.\n",
    "- Tokenization: Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. The list of tokens becomes input for further processing such as parsing or text mining.\n",
    "- Stop Words: In computing, stop words are words which are filtered out prior to, or after, processing of natural language data (text). In terms of linguistics these words are called as function words. Words like ’a’, ’an’, ’the’ are examples for stop words. There is no defined set of stop words available. Different applications and research groups uses different sets o stop words.\n",
    "Generally stop words are omitted in text mining process. The frequency of stop words will be very high in any corpus compared to content words.Pointers to some good stop word list is available at http://en.wikipedia.org/wiki/Stop_words\n",
    "- Bag of Words: The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as an un-ordered collection of words, disregarding grammar and even word order. Analyzing text by only analyzing frequency of words is called as bag of words model.\n",
    "- TF-IDF: Tf–idf, term frequency–inverse document frequency, is a numerical statistic which reflects how important a word is to a document in a collection or corpus.\n",
    "![image1](images/tfidf1.png)\n",
    "![image2](images/tfidf2.png)\n",
    "- LDA: In natural language processing, Latent Dirichlet allocation (LDA) is a generative model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics.\n",
    "\n",
    "Having set these basic fundamentals, let's try and analyze the problem at hand.\n",
    "\n",
    "###Algorithm\n",
    "####Pre-processing\n",
    "First, we have to process the raw data files. I had two options. First option was to read the XML annonated files and get the data from there. And second, I can read the raw text files. I prefered the second option for simplicity. I could not figure out the advantage of using the XML files over .txt files. But there was an computation overhead for option 1 as compared to option 2. I am skeptical that I have missed out some important feature by not using the XML files. But that's a work for another day and we can improve on the existing work later on. \n",
    "\n",
    "So now we have to recursively go into each file and read it's content. This can be done by the code below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# This is for logging data\n",
    "#\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('text_mining_logger')\n",
    "#\n",
    "# We will be appending all our documents to this array.\n",
    "#\n",
    "documents = []\n",
    "\n",
    "#\n",
    "# get the class-names from the directory structure\n",
    "#\n",
    "directory_names = list(set(glob.glob(os.path.join(\"docs\", \"*\"))\n",
    "                           ).difference(set(glob.glob(os.path.join(\"docs\", \"*.*\")))))\n",
    "#\n",
    "# List of string of class names\n",
    "#\n",
    "namesClasses = list()\n",
    "\n",
    "#\n",
    "# Navigate through the list of directories recursively\n",
    "#\n",
    "logger.info(\"Reading the documents\")\n",
    "for folder in directory_names:\n",
    "    #\n",
    "    # Append the string class name for each class\n",
    "    #\n",
    "    currentClass = folder.split(os.pathsep)[-1]\n",
    "    namesClasses.append(currentClass)\n",
    "\n",
    "    for fileNameDir in os.walk(folder):\n",
    "        for fileName in fileNameDir[2]:\n",
    "            #\n",
    "            # Only read in the text files\n",
    "            #\n",
    "            if fileName[-4:] != \".txt\":\n",
    "                continue\n",
    "            nameFileImage = \"{0}{1}{2}\".format(fileNameDir[0], os.sep, fileName)\n",
    "            with open(nameFileImage, 'r') as myfile:\n",
    "                #\n",
    "                # Read the file and remove the new-line characters.\n",
    "                #\n",
    "                data=myfile.read().replace('\\n', '')\n",
    "                #\n",
    "                # Remove all special characters.\n",
    "                #\n",
    "                new_string = re.sub('[^a-zA-Z0-9]', ' ', data)\n",
    "            documents.append(new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, I have also done some basic filtering. I have removed all special characters from the document. When I was browsing through the files, I noticed that the files contained a lot of unicode special characters and these characters don't have any special significance towards the content. Hence, I filtered these out with a simple regEX.\n",
    "\n",
    "The next step is to tokenize the words. And we also have to remove the commonly occuring words as \"a, an, is, are, will\" etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
